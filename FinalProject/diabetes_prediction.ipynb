{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "455c162b",
            "metadata": {},
            "source": [
                "# Diabetes Prediction Model\n",
                "\n",
                "This notebook implements a machine learning model to predict diabetes based on various health metrics from the NHANES dataset. It follows standard data science practices including data loading, cleaning, feature engineering, model training, and evaluation."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "43f701b1",
            "metadata": {},
            "source": [
                "## Step 1: Import Libraries\n",
                "\n",
                "Import necessary libraries for data manipulation, preprocessing, modeling, and evaluation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "id": "4a22ee66",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.model_selection import train_test_split, cross_val_score\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
                "from sklearn.utils.class_weight import compute_class_weight"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0db19079",
            "metadata": {},
            "source": [
                "## Step 2: Data Loading and Preparation\n",
                "\n",
                "Load the different NHANES datasets stored in XPT format. Each file contains specific health indicators or demographic information."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "id": "8d0dca7f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading and preparing data...\n",
                        "Data loaded successfully.\n"
                    ]
                }
            ],
            "source": [
                "print(\"Loading and preparing data...\")\n",
                "\n",
                "# Load the XPT files\n",
                "# Ensure the .xpt files are in the same directory as the notebook or provide the correct path\n",
                "try:\n",
                "    diabetes_ques = pd.read_sas('DIQ_L.xpt', format='xport')\n",
                "    fasting_glucose = pd.read_sas('GLU_L.xpt', format='xport')\n",
                "    insulin = pd.read_sas('INS_L.xpt', format='xport')\n",
                "    tot_cholesterol = pd.read_sas('TCHOL_L.xpt', format='xport')\n",
                "    weight_hist = pd.read_sas('WHQ_L.xpt', format='xport')\n",
                "    demographics = pd.read_sas('DEMO_L.xpt', format='xport')\n",
                "    print(\"Data loaded successfully.\")\n",
                "except FileNotFoundError as e:\n",
                "    print(f\"Error loading data: {e}. Make sure the XPT files are in the correct directory.\")\n",
                "    # Handle the error appropriately, e.g., stop execution or prompt user for path\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "85e8af47",
            "metadata": {},
            "source": [
                "### Select Relevant Columns\n",
                "\n",
                "Select only the necessary columns from each loaded DataFrame to keep the dataset focused and manageable. `SEQN` is the unique identifier used for merging."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "id": "047dc313",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Relevant columns selected.\n"
                    ]
                }
            ],
            "source": [
                "# Check if dataframes were loaded before proceeding\n",
                "if 'demographics' in locals():\n",
                "    demographics = demographics[['SEQN', 'RIAGENDR', 'RIDAGEYR']]\n",
                "    diabetes_ques = diabetes_ques[['SEQN', 'DIQ160']] # DIQ160: Doctor told you have diabetes\n",
                "    fasting_glucose = fasting_glucose[['SEQN', 'LBXGLU']] # Fasting Glucose (mg/dL)\n",
                "    insulin = insulin[['SEQN', 'LBDINSI']] # Insulin (uU/mL)\n",
                "    tot_cholesterol = tot_cholesterol[['SEQN', 'LBXTC']] # Total Cholesterol (mg/dL)\n",
                "    weight_hist = weight_hist[['SEQN', 'WHD010', 'WHD020']] # Height (inches), Weight (pounds)\n",
                "    print(\"Relevant columns selected.\")\n",
                "else:\n",
                "    print(\"Skipping column selection as dataframes were not loaded.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "01356fcf",
            "metadata": {},
            "source": [
                "### Feature Engineering: BMI and Overweight Status\n",
                "\n",
                "Calculate Body Mass Index (BMI) using height (`WHD010`) and weight (`WHD020`). Create a binary feature `is_overweight` based on a BMI threshold (>= 30)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "id": "906bed61",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "BMI and is_overweight calculated.\n",
                        "       SEQN  WHD010  WHD020        BMI  is_overweight\n",
                        "0  130378.0    71.0   190.0  26.496727              0\n",
                        "1  130379.0    70.0   220.0  31.563265              1\n",
                        "2  130380.0    60.0   150.0  29.291667              0\n",
                        "3  130384.0    68.0   204.0  31.014706              1\n",
                        "4  130385.0    70.0   240.0  34.432653              1\n"
                    ]
                }
            ],
            "source": [
                "# Check if weight_hist dataframe exists\n",
                "if 'weight_hist' in locals():\n",
                "    # Calculate BMI (using formula for inches and pounds)\n",
                "    # Handle potential division by zero or missing height/weight \n",
                "    weight_hist['BMI'] = np.nan # Initialize BMI column\n",
                "    # Create boolean masks for valid, non-zero, non-NaN height and weight\n",
                "    valid_height = pd.notna(weight_hist['WHD010']) & (weight_hist['WHD010'] > 0)\n",
                "    valid_weight = pd.notna(weight_hist['WHD020']) & (weight_hist['WHD020'] > 0)\n",
                "    valid_indices = valid_height & valid_weight\n",
                "    \n",
                "    weight_hist.loc[valid_indices, 'BMI'] = (weight_hist.loc[valid_indices, 'WHD020'] / \n",
                "                                             (weight_hist.loc[valid_indices, 'WHD010'] * weight_hist.loc[valid_indices, 'WHD010'])) * 703\n",
                "    \n",
                "    # Define overweight based on BMI >= 30, handle NaNs in BMI\n",
                "    weight_hist['is_overweight'] = weight_hist['BMI'].apply(lambda x: 1 if pd.notna(x) and x >= 30 else 0)\n",
                "    print(\"BMI and is_overweight calculated.\")\n",
                "    # Display first few rows with BMI calculation\n",
                "    print(weight_hist[['SEQN', 'WHD010', 'WHD020', 'BMI', 'is_overweight']].head())\n",
                "else:\n",
                "    print(\"Skipping BMI calculation as weight_hist dataframe was not loaded.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b4207314",
            "metadata": {},
            "source": [
                "### Merge Datasets\n",
                "\n",
                "Merge all the individual DataFrames into a single `final_data` DataFrame using the `SEQN` identifier. An inner merge is used to keep only participants present in all the selected columns' original datasets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "id": "d4144bc9",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Debugging Merge Check ---\n",
                        "Required DFs: ['demographics', 'fasting_glucose', 'insulin', 'tot_cholesterol', 'weight_hist']\n",
                        "Checking variables in locals():\n",
                        "Existence check: {'demographics': True, 'fasting_glucose': True, 'insulin': True, 'tot_cholesterol': True, 'weight_hist': True}\n",
                        "Types check: {'demographics': <class 'pandas.core.frame.DataFrame'>, 'fasting_glucose': <class 'pandas.core.frame.DataFrame'>, 'insulin': <class 'pandas.core.frame.DataFrame'>, 'tot_cholesterol': <class 'pandas.core.frame.DataFrame'>, 'weight_hist': <class 'pandas.core.frame.DataFrame'>}\n",
                        "Does 'all' evaluate to True?: True\n",
                        "--- Debugging Merge Check End ---\n",
                        "\n",
                        "Merging features...\n",
                        "Merged data shape: (3701, 8)\n",
                        "       SEQN  RIAGENDR  RIDAGEYR  LBXGLU  LBDINSI  LBXTC        BMI  \\\n",
                        "0  130378.0       1.0      43.0   113.0    93.18  264.0  26.496727   \n",
                        "1  130379.0       1.0      66.0    99.0   119.46  214.0  31.563265   \n",
                        "2  130380.0       2.0      44.0   156.0    97.98  187.0  29.291667   \n",
                        "3  130386.0       1.0      34.0   100.0    68.28  183.0  30.406574   \n",
                        "4  130394.0       1.0      51.0    88.0    43.20  183.0  24.963673   \n",
                        "\n",
                        "   is_overweight  \n",
                        "0              0  \n",
                        "1              1  \n",
                        "2              0  \n",
                        "3              1  \n",
                        "4              0  \n"
                    ]
                }
            ],
            "source": [
                "\n",
                "\n",
                "\n",
                "# Check if all necessary dataframes exist before merging\n",
                "required_dfs = ['demographics', 'fasting_glucose', 'insulin', 'tot_cholesterol', 'weight_hist']\n",
                "# --- DEBUG PRINTS START ---\n",
                "print(\"\\n--- Debugging Merge Check ---\")\n",
                "print(f\"Required DFs: {required_dfs}\")\n",
                "print(\"Checking variables in locals():\")\n",
                "variables_exist = {}\n",
                "variable_types = {}\n",
                "for df_name in required_dfs:\n",
                "    exists = df_name in locals()\n",
                "    variables_exist[df_name] = exists\n",
                "    if exists:\n",
                "        try:\n",
                "            # Attempt to access the variable to ensure it's valid\n",
                "            var_ref = locals()[df_name]\n",
                "            variable_types[df_name] = type(var_ref)\n",
                "        except Exception as e:\n",
                "            # Handle cases where variable exists in locals but might be problematic\n",
                "            variables_exist[df_name] = False # Mark as not truly existing for the check\n",
                "            variable_types[df_name] = f\"Error accessing: {e}\"\n",
                "    else:\n",
                "        variable_types[df_name] = None\n",
                "print(f\"Existence check: {variables_exist}\")\n",
                "print(f\"Types check: {variable_types}\")\n",
                "all_exist = all(variables_exist.values())\n",
                "print(f\"Does 'all' evaluate to True?: {all_exist}\")\n",
                "print(\"--- Debugging Merge Check End ---\\n\")\n",
                "# --- DEBUG PRINTS END ---\n",
                "\n",
                "# Make sure the next line uses the debug result:\n",
                "if all_exist:\n",
                "    print(\"Merging features...\")\n",
                "    final_data = pd.merge(demographics, fasting_glucose, on='SEQN', how='inner')\n",
                "    final_data = pd.merge(final_data, insulin, on='SEQN', how='inner')\n",
                "    final_data = pd.merge(final_data, tot_cholesterol, on='SEQN', how='inner')\n",
                "    # Merge only the relevant columns from weight_hist\n",
                "    final_data = pd.merge(final_data, weight_hist[['SEQN', 'BMI', 'is_overweight']], on='SEQN', how='inner')\n",
                "    print(f\"Merged data shape: {final_data.shape}\")\n",
                "    print(final_data.head())\n",
                "else:\n",
                "    print(\"Skipping merging as one or more required dataframes were not loaded.\")\n",
                "    final_data = None # Set to None to prevent errors in subsequent cells"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f95576e6",
            "metadata": {},
            "source": [
                "### Feature Engineering: HOMA-IR\n",
                "\n",
                "Calculate the Homeostatic Model Assessment for Insulin Resistance (HOMA-IR). This is a common indicator derived from fasting glucose and insulin levels."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "id": "a9b81709",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "HOMA-IR calculated.\n",
                        "       SEQN  LBDINSI  LBXGLU    HOMA_IR\n",
                        "0  130378.0    93.18   113.0  25.998370\n",
                        "1  130379.0   119.46    99.0  29.201333\n",
                        "2  130380.0    97.98   156.0  37.740444\n",
                        "3  130386.0    68.28   100.0  16.859259\n",
                        "4  130394.0    43.20    88.0   9.386667\n"
                    ]
                }
            ],
            "source": [
                "# Check if final_data exists and has the required columns\n",
                "if final_data is not None and all(col in final_data.columns for col in ['LBDINSI', 'LBXGLU']):\n",
                "    # Calculate HOMA-IR\n",
                "    # Handle potential division by zero or missing values\n",
                "    valid_insulin = pd.notna(final_data['LBDINSI']) & (final_data['LBDINSI'] > 0)\n",
                "    valid_glucose = pd.notna(final_data['LBXGLU']) & (final_data['LBXGLU'] > 0)\n",
                "    valid_homa_indices = valid_insulin & valid_glucose\n",
                "    \n",
                "    final_data['HOMA_IR'] = np.nan # Initialize HOMA_IR column\n",
                "    final_data.loc[valid_homa_indices, 'HOMA_IR'] = (final_data.loc[valid_homa_indices, 'LBDINSI'] * final_data.loc[valid_homa_indices, 'LBXGLU']) / 405\n",
                "    print(\"HOMA-IR calculated.\")\n",
                "    print(final_data[['SEQN', 'LBDINSI', 'LBXGLU', 'HOMA_IR']].head())\n",
                "else:\n",
                "    print(\"Skipping HOMA-IR calculation as final_data is missing or lacks required columns.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "437a05d9",
            "metadata": {},
            "source": [
                "### Define Target Variable\n",
                "\n",
                "Create the target variable `diabetes_meas`. This is defined based on the clinical guideline for fasting glucose levels: a level of 126 mg/dL or higher indicates diabetes (coded as 1), otherwise 0."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "id": "1042e351",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Target variable 'diabetes_meas' created.\n",
                        "       SEQN  LBXGLU  diabetes_meas\n",
                        "0  130378.0   113.0              0\n",
                        "1  130379.0    99.0              0\n",
                        "2  130380.0   156.0              1\n",
                        "3  130386.0   100.0              0\n",
                        "4  130394.0    88.0              0\n",
                        "\n",
                        "Value counts for target variable:\n",
                        "diabetes_meas\n",
                        "0    3271\n",
                        "1     430\n",
                        "Name: count, dtype: int64\n"
                    ]
                }
            ],
            "source": [
                "# Check if final_data exists and has the required column\n",
                "if final_data is not None and 'LBXGLU' in final_data.columns:\n",
                "    # Create diabetes measurement column based on fasting glucose levels\n",
                "    # A fasting glucose level of 126 mg/dL or higher indicates diabetes\n",
                "    final_data['diabetes_meas'] = final_data['LBXGLU'].apply(lambda x: 1 if pd.notna(x) and x >= 126 else 0)\n",
                "    print(\"Target variable 'diabetes_meas' created.\")\n",
                "    print(final_data[['SEQN', 'LBXGLU', 'diabetes_meas']].head())\n",
                "    print(\"\\nValue counts for target variable:\")\n",
                "    print(final_data['diabetes_meas'].value_counts())\n",
                "else:\n",
                "    print(\"Skipping target variable creation as final_data is missing or lacks 'LBXGLU' column.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "609b5fd5",
            "metadata": {},
            "source": [
                "## Step 3: Feature Selection and Target Definition\n",
                "\n",
                "Select the features (`X`) that will be used for modeling (Gender, Age, Glucose, Insulin, Total Cholesterol, BMI, HOMA-IR) and define the target variable (`y`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "id": "8710d513",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Preparing features (X) and target (y)...\n",
                        "Features (X) shape: (3701, 6)\n",
                        "Target (y) shape: (3701,)\n",
                        "\n",
                        "Features columns: ['RIAGENDR', 'RIDAGEYR', 'LBDINSI', 'LBXTC', 'BMI', 'HOMA_IR']\n",
                        "\n",
                        "Missing values per feature column:\n",
                        " RIAGENDR      0\n",
                        "RIDAGEYR      0\n",
                        "LBDINSI     410\n",
                        "LBXTC       376\n",
                        "BMI           0\n",
                        "HOMA_IR     432\n",
                        "dtype: int64\n"
                    ]
                }
            ],
            "source": [
                "# Check if final_data exists and has the required columns\n",
                "features_list = ['RIAGENDR', 'RIDAGEYR', 'LBDINSI', 'LBXTC', 'BMI', 'HOMA_IR']\n",
                "target = 'diabetes_meas'\n",
                "if final_data is not None and all(col in final_data.columns for col in features_list) and target in final_data.columns:\n",
                "    print(\"\\nPreparing features (X) and target (y)...\")\n",
                "    X = final_data[features_list]\n",
                "    y = final_data[target]\n",
                "    print(\"Features (X) shape:\", X.shape)\n",
                "    print(\"Target (y) shape:\", y.shape)\n",
                "    print(\"\\nFeatures columns:\", X.columns.tolist())\n",
                "    print(\"\\nMissing values per feature column:\\n\", X.isnull().sum())\n",
                "else:\n",
                "    print(\"Skipping feature/target selection as final_data is missing or lacks required columns.\")\n",
                "    X = None\n",
                "    y = None"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e24af668",
            "metadata": {},
            "source": [
                "## Step 4: Handle Missing Values\n",
                "\n",
                "Use `SimpleImputer` to fill missing values in the feature set `X`. The 'mean' strategy replaces NaNs with the mean of each respective column. This is crucial because many models cannot handle missing data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "id": "1ccd829b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Handling missing values using mean imputation...\n",
                        "Missing values after imputation:\n",
                        " RIAGENDR    0\n",
                        "RIDAGEYR    0\n",
                        "LBDINSI     0\n",
                        "LBXTC       0\n",
                        "BMI         0\n",
                        "HOMA_IR     0\n",
                        "dtype: int64\n",
                        "\n",
                        "First 5 rows of imputed features:\n",
                        "   RIAGENDR  RIDAGEYR  LBDINSI  LBXTC        BMI    HOMA_IR\n",
                        "0       1.0      43.0    93.18  264.0  26.496727  25.998370\n",
                        "1       1.0      66.0   119.46  214.0  31.563265  29.201333\n",
                        "2       2.0      44.0    97.98  187.0  29.291667  37.740444\n",
                        "3       1.0      34.0    68.28  183.0  30.406574  16.859259\n",
                        "4       1.0      51.0    43.20  183.0  24.963673   9.386667\n"
                    ]
                }
            ],
            "source": [
                "# Check if X is available\n",
                "if X is not None:\n",
                "    print(\"Handling missing values using mean imputation...\")\n",
                "    imputer = SimpleImputer(strategy='mean')\n",
                "    X_imputed = imputer.fit_transform(X)\n",
                "    # Convert back to DataFrame to retain column names and check results\n",
                "    X = pd.DataFrame(X_imputed, columns=X.columns)\n",
                "    print(\"Missing values after imputation:\\n\", X.isnull().sum())\n",
                "    print(\"\\nFirst 5 rows of imputed features:\")\n",
                "    print(X.head())\n",
                "else:\n",
                "    print(\"Skipping imputation as features (X) are not available.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "95009fd7",
            "metadata": {},
            "source": [
                "## Step 5: Train-Test Split and Standardization\n",
                "\n",
                "Split the data into training and testing sets (80% train, 20% test). `stratify=y` ensures that the proportion of the target variable classes is maintained in both sets, which is important for imbalanced datasets.\n",
                "Standardize the features using `StandardScaler`. This scales the data to have a mean of 0 and a standard deviation of 1. Standardization is beneficial for algorithms sensitive to feature scales, like Logistic Regression and SVM."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "id": "c3f671fc",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Splitting data into training and testing sets...\n",
                        "Training set shape: X=(2960, 6), y=(2960,)\n",
                        "Testing set shape: X=(741, 6), y=(741,)\n",
                        "\n",
                        "Training set target distribution:\n",
                        " diabetes_meas\n",
                        "0    0.883784\n",
                        "1    0.116216\n",
                        "Name: proportion, dtype: float64\n",
                        "\n",
                        "Testing set target distribution:\n",
                        " diabetes_meas\n",
                        "0    0.883941\n",
                        "1    0.116059\n",
                        "Name: proportion, dtype: float64\n",
                        "\n",
                        "Scaling features using StandardScaler...\n",
                        "Features scaled successfully.\n",
                        "\n",
                        "First 5 rows of scaled training features:\n",
                        "   RIAGENDR  RIDAGEYR   LBDINSI     LBXTC       BMI   HOMA_IR\n",
                        "0  0.911345 -0.081229 -0.570802  0.059609 -0.141960 -0.408442\n",
                        "1 -1.097279  1.490938 -0.269461 -0.728233 -0.120506 -0.227610\n",
                        "2 -1.097279 -0.605284  0.133400  1.512193 -0.042404  0.006925\n",
                        "3  0.911345 -1.548585 -0.322287 -0.826714 -0.096522 -0.283005\n",
                        "4 -1.097279 -0.133634  0.001522 -0.009927 -0.004516 -0.000709\n"
                    ]
                }
            ],
            "source": [
                "# Check if X and y are available\n",
                "if X is not None and y is not None:\n",
                "    print(\"Splitting data into training and testing sets...\")\n",
                "    # Check if there are enough samples in each class for stratification\n",
                "    if np.min(y.value_counts()) >= 2: # Need at least 2 samples per class for stratify\n",
                "      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
                "      print(f\"Training set shape: X={X_train.shape}, y={y_train.shape}\")\n",
                "      print(f\"Testing set shape: X={X_test.shape}, y={y_test.shape}\")\n",
                "      print(\"\\nTraining set target distribution:\\n\", y_train.value_counts(normalize=True))\n",
                "      print(\"\\nTesting set target distribution:\\n\", y_test.value_counts(normalize=True))\n",
                "      \n",
                "      print(\"\\nScaling features using StandardScaler...\")\n",
                "      scaler = StandardScaler()\n",
                "      X_train_scaled = scaler.fit_transform(X_train)\n",
                "      X_test_scaled = scaler.transform(X_test)\n",
                "      print(\"Features scaled successfully.\")\n",
                "      # Convert scaled arrays back to DataFrames for potential inspection (optional)\n",
                "      X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
                "      X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
                "      print(\"\\nFirst 5 rows of scaled training features:\")\n",
                "      print(X_train_scaled.head())\n",
                "    else:\n",
                "      print(\"Skipping train-test split due to insufficient samples in minority class for stratification.\")\n",
                "      X_train, X_test, y_train, y_test = (None, None, None, None)\n",
                "      X_train_scaled, X_test_scaled = (None, None)\n",
                "else:\n",
                "    print(\"Skipping train-test split and scaling as features (X) or target (y) are not available.\")\n",
                "    X_train, X_test, y_train, y_test = (None, None, None, None)\n",
                "    X_train_scaled, X_test_scaled = (None, None)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "44fa40e2",
            "metadata": {},
            "source": [
                "## Step 6: Model Training with Class Weighting\n",
                "\n",
                "Address class imbalance (observed in the target variable value counts) by calculating class weights. The 'balanced' mode automatically adjusts weights inversely proportional to class frequencies. This gives more importance to the minority class during training, helping the model learn its patterns better.\n",
                "Initialize several common classification models: Logistic Regression, Random Forest, Gradient Boosting, and Support Vector Machine (SVM). Apply the calculated class weights to models that support this parameter (Logistic Regression, Random Forest, SVM) to mitigate bias towards the majority class."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "id": "e3e365f0",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Calculating class weights for model training...\n",
                        "Calculated Class Weights: {0: 0.5657492354740061, 1: 4.3023255813953485}\n",
                        "\n",
                        "Initializing models...\n",
                        "Initialized models: ['Logistic Regression', 'Random Forest', 'Gradient Boosting', 'SVM']\n"
                    ]
                }
            ],
            "source": [
                "# Check if training data is available\n",
                "models = {}\n",
                "class_weight_dict = None\n",
                "if X_train_scaled is not None and y_train is not None:\n",
                "    print(\"\\nCalculating class weights for model training...\")\n",
                "    # Check if y_train has more than one class before computing weights\n",
                "    unique_classes = np.unique(y_train)\n",
                "    if len(unique_classes) > 1:\n",
                "        class_weights = compute_class_weight('balanced', classes=unique_classes, y=y_train)\n",
                "        class_weight_dict = dict(zip(unique_classes, class_weights))\n",
                "        print(f\"Calculated Class Weights: {class_weight_dict}\")\n",
                "    else:\n",
                "        print(f\"Only one class ({unique_classes[0]}) present in y_train. Class weighting not applicable.\")\n",
                "        class_weight_dict = None \n",
                "\n",
                "    print(\"\\nInitializing models...\")\n",
                "    # Initialize models with class weights (where applicable)\n",
                "    models = {\n",
                "        'Logistic Regression': LogisticRegression(max_iter=1000, class_weight=class_weight_dict, random_state=42),\n",
                "        'Random Forest': RandomForestClassifier(n_estimators=100, class_weight=class_weight_dict, random_state=42),\n",
                "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42), # GB needs sample_weight in fit\n",
                "        'SVM': SVC(probability=True, class_weight=class_weight_dict, random_state=42)\n",
                "    }\n",
                "    print(f\"Initialized models: {list(models.keys())}\")\n",
                "else:\n",
                "    print(\"Skipping model initialization as training data is not available.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "36f7b399",
            "metadata": {},
            "source": [
                "## Step 7: Model Training and Evaluation\n",
                "\n",
                "Iterate through the initialized models:\n",
                "1.  **Train** each model on the scaled training data (`X_train_scaled`, `y_train`). For Gradient Boosting, if class weighting is desired, `sample_weight` must be passed to the `fit` method.\n",
                "2.  **Predict** on the scaled test data (`X_test_scaled`).\n",
                "3.  **Evaluate** using:\n",
                "    *   Accuracy: Overall proportion of correct predictions.\n",
                "    *   Classification Report: Shows precision, recall, and F1-score for each class. Precision is the accuracy of positive predictions. Recall (Sensitivity) is the ability to find all positive samples. F1-score is the harmonic mean of precision and recall.\n",
                "    *   Confusion Matrix: Shows True Positives, False Positives, True Negatives, and False Negatives."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "id": "b51cd415",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Training and Evaluating Models ---\n",
                        "\n",
                        "Training Logistic Regression...\n",
                        "\n",
                        "--- Evaluation for Logistic Regression ---\n",
                        "Accuracy: 0.9420\n",
                        "\n",
                        "Classification Report:\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "           0       0.99      0.95      0.97       655\n",
                        "           1       0.69      0.91      0.78        86\n",
                        "\n",
                        "    accuracy                           0.94       741\n",
                        "   macro avg       0.84      0.93      0.88       741\n",
                        "weighted avg       0.95      0.94      0.95       741\n",
                        "\n",
                        "\n",
                        "Confusion Matrix:\n",
                        "[[620  35]\n",
                        " [  8  78]]\n",
                        "\n",
                        "Training Random Forest...\n",
                        "\n",
                        "--- Evaluation for Random Forest ---\n",
                        "Accuracy: 0.9244\n",
                        "\n",
                        "Classification Report:\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "           0       0.93      0.99      0.96       655\n",
                        "           1       0.83      0.44      0.58        86\n",
                        "\n",
                        "    accuracy                           0.92       741\n",
                        "   macro avg       0.88      0.71      0.77       741\n",
                        "weighted avg       0.92      0.92      0.91       741\n",
                        "\n",
                        "\n",
                        "Confusion Matrix:\n",
                        "[[647   8]\n",
                        " [ 48  38]]\n",
                        "\n",
                        "Training Gradient Boosting...\n",
                        "\n",
                        "--- Evaluation for Gradient Boosting ---\n",
                        "Accuracy: 0.9406\n",
                        "\n",
                        "Classification Report:\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "           0       0.98      0.95      0.97       655\n",
                        "           1       0.69      0.88      0.78        86\n",
                        "\n",
                        "    accuracy                           0.94       741\n",
                        "   macro avg       0.84      0.92      0.87       741\n",
                        "weighted avg       0.95      0.94      0.94       741\n",
                        "\n",
                        "\n",
                        "Confusion Matrix:\n",
                        "[[621  34]\n",
                        " [ 10  76]]\n",
                        "\n",
                        "Training SVM...\n",
                        "\n",
                        "--- Evaluation for SVM ---\n",
                        "Accuracy: 0.9123\n",
                        "\n",
                        "Classification Report:\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "           0       0.97      0.93      0.95       655\n",
                        "           1       0.59      0.79      0.68        86\n",
                        "\n",
                        "    accuracy                           0.91       741\n",
                        "   macro avg       0.78      0.86      0.81       741\n",
                        "weighted avg       0.93      0.91      0.92       741\n",
                        "\n",
                        "\n",
                        "Confusion Matrix:\n",
                        "[[608  47]\n",
                        " [ 18  68]]\n"
                    ]
                }
            ],
            "source": [
                "# Check if models and test data are available\n",
                "results = {}\n",
                "if models and X_test_scaled is not None and y_test is not None:\n",
                "    print(\"\\n--- Training and Evaluating Models ---\")\n",
                "    for name, model in models.items():\n",
                "        print(f\"\\nTraining {name}...\")\n",
                "        try:\n",
                "            # Special handling for Gradient Boosting sample weights\n",
                "            if name == 'Gradient Boosting' and class_weight_dict and len(np.unique(y_train)) > 1:\n",
                "                sample_weights = np.array([class_weight_dict[label] for label in y_train])\n",
                "                model.fit(X_train_scaled, y_train, sample_weight=sample_weights)\n",
                "            else:\n",
                "                 # Standard fit for other models or GB without weights\n",
                "                model.fit(X_train_scaled, y_train)\n",
                "            \n",
                "            # Predict and Evaluate\n",
                "            y_pred = model.predict(X_test_scaled)\n",
                "            accuracy = accuracy_score(y_test, y_pred)\n",
                "            results[name] = accuracy\n",
                "            \n",
                "            print(f\"\\n--- Evaluation for {name} ---\")\n",
                "            print(f\"Accuracy: {accuracy:.4f}\")\n",
                "            print(\"\\nClassification Report:\")\n",
                "            # Use zero_division=0 to avoid warnings when a class has no predicted samples\n",
                "            print(classification_report(y_test, y_pred, zero_division=0))\n",
                "            print(\"\\nConfusion Matrix:\")\n",
                "            cm = confusion_matrix(y_test, y_pred)\n",
                "            print(cm)\n",
                "            # Optional: Nicer confusion matrix display\n",
                "            # import seaborn as sns\n",
                "            # import matplotlib.pyplot as plt\n",
                "            # sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
                "            # plt.xlabel('Predicted')\n",
                "            # plt.ylabel('Actual')\n",
                "            # plt.title(f'{name} Confusion Matrix')\n",
                "            # plt.show()\n",
                "\n",
                "        except Exception as e:\n",
                "             print(f\"Error training or evaluating {name}: {e}\")\n",
                "             results[name] = None # Mark as failed\n",
                "else:\n",
                "    print(\"Skipping model training and evaluation as models or test data are not available.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fb802696",
            "metadata": {},
            "source": [
                "### Identify Best Model\n",
                "\n",
                "Determine the model with the highest accuracy score based on the evaluation results stored in the `results` dictionary."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 59,
            "id": "5f8b5a46",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Best Performing Model ---\n",
                        "Model: Logistic Regression\n",
                        "Accuracy: 0.9420\n"
                    ]
                }
            ],
            "source": [
                "best_model_name = None\n",
                "best_model = None\n",
                "# Filter out models that failed (results[name] is None)\n",
                "valid_results = {name: acc for name, acc in results.items() if acc is not None}\n",
                "\n",
                "if valid_results: \n",
                "    best_model_name = max(valid_results, key=valid_results.get)\n",
                "    print(f\"\\n--- Best Performing Model ---\")\n",
                "    print(f\"Model: {best_model_name}\")\n",
                "    print(f\"Accuracy: {valid_results[best_model_name]:.4f}\")\n",
                "    # Retrieve the best model instance from the dictionary\n",
                "    if best_model_name in models:\n",
                "        best_model = models[best_model_name]\n",
                "else:\n",
                "    print(\"\\nNo models were successfully trained and evaluated to determine the best one.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0578d45d",
            "metadata": {},
            "source": [
                "### Feature Importance (for Tree-Based Models)\n",
                "\n",
                "If the best performing model is a tree-based ensemble (like Random Forest or Gradient Boosting), examine its `feature_importances_` attribute. This shows the relative importance of each feature in making predictions, helping to understand the drivers of the model's decisions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "id": "9661fd31",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Feature importance not available for Logistic Regression.\n"
                    ]
                }
            ],
            "source": [
                "# Check if the best model is tree-based and has feature importances\n",
                "if best_model_name in ['Random Forest', 'Gradient Boosting'] and best_model is not None and hasattr(best_model, 'feature_importances_'):\n",
                "    print(f\"\\n--- Feature Importance for {best_model_name} ---\")\n",
                "    # Ensure X (features DataFrame) is available for column names\n",
                "    if X is not None:\n",
                "        feature_importance = pd.DataFrame({\n",
                "            'Feature': X.columns,\n",
                "            'Importance': best_model.feature_importances_\n",
                "        })\n",
                "        # Sort by importance descending\n",
                "        feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
                "        print(feature_importance)\n",
                "        \n",
                "        # Optional: Plot feature importances\n",
                "        # import matplotlib.pyplot as plt\n",
                "        # import seaborn as sns\n",
                "        # plt.figure(figsize=(10, 6))\n",
                "        # sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
                "        # plt.title(f'{best_model_name} Feature Importance')\n",
                "        # plt.show()\n",
                "    else:\n",
                "        print(\"Cannot display feature names as the features DataFrame (X) is not available.\")\n",
                "elif best_model_name:\n",
                "     print(f\"\\nFeature importance not available for {best_model_name}.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c40091fc",
            "metadata": {},
            "source": [
                "## Step 8: Cross-Validation\n",
                "\n",
                "Perform 5-fold cross-validation on the *best-performing model* identified previously, using the *training data* (`X_train_scaled`, `y_train`). Cross-validation provides a more robust estimate of the model's performance and generalization ability compared to a single train-test split. It involves splitting the training data into multiple folds, training the model on some folds, and evaluating it on the remaining fold, repeating this process until each fold has served as the evaluation set.\n",
                "\n",
                "**Note:** If Gradient Boosting was the best model and used `sample_weight` during initial training, these weights should ideally also be passed during cross-validation using `fit_params` for a consistent evaluation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 61,
            "id": "e98d8592",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Cross-Validation on Best Model ---\n",
                        "Performing 5-fold cross-validation for: Logistic Regression...\n",
                        "\n",
                        "Cross-validation Accuracy Scores: [0.93412162 0.91891892 0.92060811 0.93074324 0.94763514]\n",
                        "Mean CV Accuracy: 0.9304\n",
                        "Standard Deviation of CV Accuracy: 0.0104\n",
                        "Approx. 95% Confidence Interval: 0.9096 - 0.9512\n"
                    ]
                }
            ],
            "source": [
                "print(\"\\n--- Cross-Validation on Best Model ---\")\n",
                "\n",
                "# Check if a best model was identified and if CV is possible (requires >1 class)\n",
                "if best_model is not None and y_train is not None and len(np.unique(y_train)) > 1:\n",
                "    print(f\"Performing 5-fold cross-validation for: {best_model_name}...\")\n",
                "    \n",
                "    # Re-initialize the model to ensure fresh state for CV\n",
                "    # Get the configuration of the best model\n",
                "    best_model_config = best_model.get_params() \n",
                "    cv_model = type(best_model)(**best_model_config) # Create a new instance\n",
                "    \n",
                "    # Prepare fit_params for sample_weight if needed (e.g., for GB)\n",
                "    fit_params = {}\n",
                "    if best_model_name == 'Gradient Boosting' and class_weight_dict:\n",
                "        print(\"Applying sample weights during Gradient Boosting cross-validation.\")\n",
                "        sample_weights_cv = np.array([class_weight_dict[label] for label in y_train])\n",
                "        fit_params['sample_weight'] = sample_weights_cv\n",
                "        \n",
                "    try:\n",
                "        # Perform cross-validation\n",
                "        cv_scores = cross_val_score(\n",
                "            cv_model, \n",
                "            X_train_scaled, \n",
                "            y_train, \n",
                "            cv=5, \n",
                "            scoring='accuracy', # Can use other metrics like 'f1', 'roc_auc'\n",
                "            fit_params=fit_params if fit_params else None,\n",
                "            error_score='raise' # Raise error if a fold fails\n",
                "        )\n",
                "        print(f\"\\nCross-validation Accuracy Scores: {cv_scores}\")\n",
                "        print(f\"Mean CV Accuracy: {cv_scores.mean():.4f}\")\n",
                "        print(f\"Standard Deviation of CV Accuracy: {cv_scores.std():.4f}\")\n",
                "        # Confidence interval approximation\n",
                "        print(f\"Approx. 95% Confidence Interval: {cv_scores.mean() - 2*cv_scores.std():.4f} - {cv_scores.mean() + 2*cv_scores.std():.4f}\")\n",
                "        \n",
                "    except ValueError as e:\n",
                "        print(f\"\\nError during cross-validation: {e}\")\n",
                "        print(\"This might occur if a fold contains only one class, especially with small datasets or high imbalance.\")\n",
                "    except Exception as e:\n",
                "        print(f\"\\nAn unexpected error occurred during cross-validation: {e}\")\n",
                "\n",
                "elif best_model is None:\n",
                "    print(\"Skipping cross-validation as no best model was determined.\")\n",
                "elif y_train is None:\n",
                "     print(\"Skipping cross-validation as training target (y_train) is not available.\")\n",
                "else: # Only one class in y_train\n",
                "    print(\"Skipping cross-validation as there is only one class in the training data.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
